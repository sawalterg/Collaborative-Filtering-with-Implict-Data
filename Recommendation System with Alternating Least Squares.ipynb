{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering with Alternating Least Squares on the Million Song Dataset\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In 2016, more than half of all Americans consumed music through online streaming services on a weekly basis. Online streaming services generated $17.3 billion in revenue in 2017, up 8.1% from 2016. As more listening moves from mp3, AM/FM and physical mediums to streaming, online radio firms are relying on machine learning to rapidly personalize music selection and discovery for users in order to gain a competitive advantage in growing marketplace. Companies such as Spotify and Google have purchased smaller technology start-ups for their audio machine learning solutions (EchoNest) or their extensive customer data (Songza) to get leg up in this digital arms race.\n",
    "\n",
    "Streaming companies utilize a variety of methods to recommend music to their users. Pandora allows users to give songs  a “thumbs up or down” to indicate positive or negative preference. This explicit feedback helps Pandora quickly create personalized playlists, where songs with similar attributes are served to the user based off their rating feedback. Spotify generates playlists based off your listeneing history and factors in everything from time of day to location. With machine learning, a streaming company can use a variety of data to infer a listener’s taste and recommend songs the user is more likely to prefer automatically. Using recommendation systems, companies can create personalized playlists and radio stations without employing taste-makers or experts. \n",
    "\n",
    "### Recommendation Systems\n",
    "\n",
    "Recommendation systems can be broken into two categories: content-based and collaborative filters. Content-based systems rely on attributes related to the item/product (a song or artist in this case) to make recommendations to a user based off known user data. While this apprach is effective, it requres storing a large amount of data about the users and items. Often times, companies don't always have customer data to create this type of recommendation system.\n",
    "    \n",
    "A collaborative filter creates a recommendation engine using a user identifier, an item identifier and some rating metric that shows an interaction between the user and item. Based off a collection these user/item interactions, the collaborative filter can predict a user's potential item preference based off users with similar tastes. Similarly, a collaborative filter can infer item similarities as well. \n",
    "        \n",
    "Collaborative filters can deal with two types ratings data: explicit or implicit. Explicit data means a user has directly made their preference known--similar to Pandora's thumbs up or thumbs down or a 5-star rating system like Yelp's restaurant reviews. Implicit ratings are a feedback metric does not necessarily give us a user's preference, but we can potentially infer preference--like a product purchase, time spent on a webpage or a song play. A using song plays as an example, a single listen to a song does not necessarily mean the user likes that song, but the more times a user listens to that same song, we can more confidently infer the user has a positive preference for said song. While implicit metrics sacrifice a degree of certainty, implict data is much easier to come by in the real world. \n",
    "    \n",
    "    \n",
    "### The Task\n",
    "\n",
    "The Million Song Challenge (MSC) data set is ideal for creating a recommendation system. Moreover, its format allows us to create a collaborative filter using implicit data. The dataset itself contains over 1 million unique user’s listening data and over 300,000 songs. The task is to create a robust recommendation system with just the interaction between users and songs. As a basis of comparison, we will be comparing the performance of our recommender model to serving the most popular song any given user has not listened to. This is a high bar to clear, as recommending the most popular item tends have a higher accuracy than a recommendation system\n",
    "       \n",
    "Given the structure of the dataset, which is sparse--as most users will have only listened to several songs, and we are dealing with implicit ratings data, I will use an Alternating Least Squares (ALS) algorithm with matrix factorization. \n",
    "    \n",
    "### Matrix Factorization\n",
    "\n",
    "Matrix factorization is a form of latent factor analysis. We start with a sparse matrix M which consists of users by items with ratings metric (song plays) in the cells. With matrix factorization, we factor matrix M into two separate matrices, X and Y; these matrices contain latent factors which reduce large numbers of interactions between users and items into several hidden or unobserved reasons. The X matrix is the users by these hidden features while the Y matrix is the items * hidden factors. Both matrices have a observations which  correspond with either a user or feature from the original matrix M. We then take the product of these matrices to fill in the missing entries of M to provide the liklihood of a user prefering an item.\n",
    "    \n",
    "$$M = XY^T$$\n",
    "    \n",
    "The ALS algorithm is used to find, or approximate, X and Y. Both X and Y aren't known, so values are randomly initialized, first for Y. Then X is solved given the random values of Y and M. Using linear algebra, the algorithm solves for each row of X (i).\n",
    "    \n",
    "$$M_1Y(Y^TY)^{-1} = X_i$$\n",
    "\n",
    "Since the algorithm will not arrive at the exact equivalent, since X and Y are too small/low rank to equal M, values are optimized by minimizing the sum of squared differences between the both matrix entries. \n",
    "    \n",
    "In an implicit scenario, our original matrix M is converted to a binary preference matrix, matrix B. Matrix M is kept to incorporate the positive values as weights. For example, a user who listened to a song 100 times will be said to have more preference than someone who listened to a song 2 times.The equation for deriving the confidence matrix is below:\n",
    "\n",
    "$$C_{ui} = 1 + \\alpha r _{ui}$$\n",
    "\n",
    "\n",
    "$$C_{ui}$$ is the confidence matrix for users and items. $$\\alpha$$ is a scaling value which controls the weight of observed vs. unobserved interactions in the factorization process. $$ r_{ui} is the original matrix, M.\n",
    "\n",
    "ALS minimizes the cost function with the following equation below\n",
    "\n",
    "$$x_u = (Y^TC^uY + \\lambda I)^{-1}Y^TC^up(u)$$\n",
    "\n",
    "$$p(u)$$ is the vector of all preferences by u (user) in in binary form. $$\\lambda$$ is the regularization term to avoid overfitting in the training phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the relevent libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import implicit\n",
    "import random\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the training and test data\n",
    "\n",
    "# Training data\n",
    "train = pd.read_csv('train_triplets.txt', delimiter = '\\t', header = None)\n",
    "train.columns = ['users', 'songs', 'play count']\n",
    "\n",
    "# Test data\n",
    "test = pd.read_csv('kaggle_visible_evaluation_triplets.txt', delimiter = '\\t', header = None, names = ['users', 'songs', 'play_count'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split?\n",
    "\n",
    "Typical machine learning problems involve training an algorithm on a one dataset and then testing the model on a hold-out dataset. This approach will not work here, as we need all possible user-item interactions to optimize our latent factors vectors. We want to reduce the amount of sparsity as much as possible. Instead we will mask a certain percentage of user-item interactions and see if our algorithm correctly predicted if a user preferred that song.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combining the two dataframe\n",
    "\n",
    "df = pd.concat([train, test], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track information \n",
    "\n",
    "There is a separate .txt file which maps the song ID from the main dataset to the actual song name and artist name. This information will be useful later when analyzing the results of the data. Viewing the recommendations as songIDs will not be as intuitive as viewing them as a collection of recongnizable songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import track and artist information for later analysis\n",
    "\n",
    "track_info = pd.read_csv('unique_tracks.txt', header = None, delimiter = '\\<SEP>', names = ['track_id', 'songs', 'artist', 'song_title'] )\n",
    "\n",
    "# drop duplicates by songID\n",
    "\n",
    "track_info_unique = track_info.drop_duplicates(subset = ['songs'])\n",
    "\n",
    "track_info.song_artist = track_info['song_title'] + (\" - \") + track_info['artist']\n",
    "\n",
    "# merge dataframe\n",
    "\n",
    "df_track = df.merge(track_info_unique[['songs', 'song_artist']], on = 'songs', how = 'inner')\n",
    "\n",
    "# check first and last 5 entries of new dataframe\n",
    "\n",
    "df_track.head()\n",
    "df_track.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Unlike more traditional machine learning tasks, the exploratory data analysis phase is limited, since there are relatively few fields. There is still value in performing basic exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_track['play count'].describe().apply(lambda x: format(x, 'f')))\n",
    "\n",
    "# How many unique users\n",
    "\n",
    "print(\"Number of unique users: \" + str(len(df_track['users'].drop_duplicates())))\n",
    "\n",
    "# How many unique songs\n",
    "\n",
    "print(\"Number of unique songs: \" + str(len(df_track['songs'].drop_duplicates())))\n",
    "\n",
    "# Check for null values\n",
    "\n",
    "print(\"Number of null ratings: \" + str(df_track['play_count'].isnull().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check top played songs\n",
    "\n",
    "song_pivot = df_track.pivot_table(index = 'song_artist',\n",
    "                                  values = \"play_count\",\n",
    "                                  aggfunc = sum)\n",
    "\n",
    "print(song_pivot.sort_values('play_count', ascending= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "\n",
    "print(df_track['play_count'].quantile(np.arange(0,1,0.05)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot histogram for distribution\n",
    "_ = plt.hist(df_track['play_count'], bins = 10, range = [play_count_min, 40], normed = True)\n",
    "_ = plt.xlabel('Play counts')\n",
    "_ = plt.ylabel('Counts')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The ALS algorithm requires that our data be in the form of a sparse matrix of users * items with interactions (play count) in the cells. Therefore, we will create a sparse matrix with the relevent columns. We must also encode the user and item variables for faster processing. We will map these codes to the song title and artist name for interpreting our model at a later point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode relevent variables\n",
    "\n",
    "\n",
    "df_track['user_id'] = df_track['users'].astype(\"category\").cat.codes\n",
    "df_track['song_id'] = df_track['songs'].astype(\"category\").cat.codes\n",
    "\n",
    "# Create lookup for songs and artists based off encodes variables\n",
    "\n",
    "song_map = df_track[['songs', 'song_artist']].drop_duplicates()\n",
    "song_map['songs'] = song_map['songs'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sparse matrix\n",
    "\n",
    "sp_item_user = sp.csr_matrix((df_track['play_count'].astype(float), (df_track['user_id'], df_track['song_id'])))\n",
    "sp_user_item = sp.csr_matrix((df_track['play_count'].astype(float), (df_track['song_id'], df_track['user_id'])))\n",
    "\n",
    "# Calculate density\n",
    "\n",
    "sparsity = 1-(df_track.shape[0] / (df_track.user_id.unique().shape[0] * df_track.song_id.unique().shape[0]))\n",
    "print(sparsity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Recommendation Efficacy\n",
    "\n",
    "As previously mentioned, using a traditional test-train split will not work for our purposes. We will use a function that masks a certain percentage of user-item interactions on a validation set of data. After creating our recommender, we will check to see if our model ended up serving songs the user already listened to in the masked entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_mask(sp_matrix, test_split = 0.20):\n",
    "  \n",
    "    val_set = sp_matrix.copy() # make test set from train data \n",
    "    val_set[val_set != 0] = 1 # Create unary matrix, all interactions\n",
    "    train = sp_matrix.copy() # train set where certain interactions will be hidden\n",
    "    nonzero_inds = train.nonzero() # use nonzero method to store user/items with interactions\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # list of user/song interactions that aren't zero\n",
    "    random.seed(123) # set random seed for reproducability\n",
    "    mask_number = int(np.rint(test_split*len(nonzero_pairs))) #  # of samples\n",
    "    masked = random.sample(nonzero_pairs, mask_number) # Sample a random number of user-item pairs (no replacement)\n",
    "    user_inds = [index[0] for index in masked] # All user row indices\n",
    "    item_inds = [index[1] for index in masked] # All song column indices\n",
    "    train[user_inds, item_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero thereby masking them\n",
    "    train.eliminate_zeros() # Delete zeros in sparse array storage after update to save space\n",
    "    return train, val_set, list(set(user_inds)) # \n",
    "\n",
    "train, val_set, user_list_train = train_test_mask(sp_item_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "A python package called Implicit is a great out-of-the-box option for creating a collaborative filter using the ALS method. This package uses Cython for processing code with different threads and the syntax is very straightforward. The hyperparameters are alpha, which is a scaling value for our ratings matrix, regularization, an overfitting parameter, number of factors, the number of hidden or latent factors and iterations, which is the number of times to switch between the user and item matrices in the alternating least squares process. We used the basic parameters here (hyperparamter tuning is restricted here due to memory constraints--see next section in Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use alternating Least Squares to create recommendation algorithm\n",
    "\n",
    "alpha = 15\n",
    "rec = implicit.als.AlternatingLeastSquares(factors = 50, regularization = 0.1,\n",
    "                                           iterations = 20)\n",
    "rec.fit(train * alpha)\n",
    "\n",
    "# Create latent features object for users and items respectively \n",
    "user_vec, item_vec = rec.item_factors, rec.user_factors.transpose() # transpose for matrix operations\n",
    "\n",
    "\n",
    "# Spot check recommendations for a user\n",
    "\n",
    "# Choose user (arbitrary)\n",
    "\n",
    "user = 22\n",
    "\n",
    "# Function to return a dataframe of song recommendations\n",
    "\n",
    "def user_recommendations(user, sp_matrix):\n",
    "    songs = []\n",
    "    scores = []\n",
    "    recommended_songs = rec.recommend(user, sp_matrix)\n",
    "    for song in recommended_songs:\n",
    "        idx, score = song\n",
    "        songs.append(df_track.song_artist.loc[df_track.song_id == idx].iloc[0])\n",
    "        scores.append(score)\n",
    "        \n",
    "        recommendations = pd.DataFrame({'songs': songs, 'score': score})\n",
    "    return recommendations\n",
    "\n",
    "user_recommdations(user, sp_user_item)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Recommendation Efficacy continued\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Area under the curve (AUC) score function\n",
    "\n",
    "def auc_score(pred, actual):\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual, pred)\n",
    "    return metrics.auc(fpr, tpr)  \n",
    "\n",
    "\n",
    "\n",
    "# Creating ROC curve and evaluating our model against a naive model which serves the most popular song the user has not previously listened to\n",
    "# by comparing the AUC scores\n",
    "\n",
    "def area_under_curve(training_set, user_list, predictions, val_set):\n",
    "\n",
    "    \n",
    "    als_auc = [] # list for item that\n",
    "    popularity_auc = [] # To store popular AUC scores\n",
    "    pop_items = np.array(val_set.sum(axis = 0)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "    item_vecs = predictions[1]\n",
    "    for index, user in enumerate(user_list): # Iterate through each user that had an item altered\n",
    "        print('Loop: {}'.format(index) + ' out of {}'.format(len(user_list))) # Note which part of the loop we are on (this will take a while)\n",
    "        training_row = training_set[user,:].toarray().reshape(-1) # Get the training set row\n",
    "        zero_inds = np.where(training_row == 0) # Find where the interaction had not yet occurred\n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user,:]\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = val_set[user,:].toarray()[0,zero_inds].reshape(-1) \n",
    "        # take the unary 1/0 interaction pairs from the original matrix\n",
    "        # which align with the same pairs in train set \n",
    "        pop = pop_items[zero_inds] # Get most popular item\n",
    "        als_auc.append(auc_score(pred, actual)) # Calculate AUC for the given user\n",
    "        popularity_auc.append(auc_score(pop, actual)) # Calculate AUC using most popular\n",
    "        \n",
    "    \n",
    "    return float('%.3f'%np.mean(als_auc)), float('%.3f'%np.mean(popularity_auc))  \n",
    "   # Return the mean AUC for our model and the most popular item for comparison\n",
    "\n",
    "area_under_curve(train, user_list_train, \n",
    "              [sp.csr_matrix(user_vec), sp.csr_matrix(item_vec)], val_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning our Model\n",
    "\n",
    "Our model did not outperform serving the most popular songs but still scored high. As mentioned in the beginning, popularity based models tend to outperform basic recommendation systems. In order to tune our model further, we will reduce the sparsity of our model by removing users and items below a certain threshold of interaction. A rule of thumb when dealing with sparse data in a recommender system is we do not want sparsity to exceed 99.5%. Therefore we will remove all users who have listened below x number of songs and songs not listened to at least x number of times until our model reaches meets this threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activity_thresh(df, user_min, song_min):\n",
    "    while True:\n",
    "        start_dim = df.shape[0]\n",
    "        song_counts = df.groupby('user_id').song_id.count()\n",
    "        df = df[~df.user_id.isin(song_counts[song_counts < song_min].index.tolist())]\n",
    "        user_counts = df.groupby('song_id').user_id.count()\n",
    "        df = df[~df.song_id.isin(user_counts[user_counts < user_min].index.tolist())]\n",
    "        end_dim = df.shape[0]\n",
    "        if start_dim == end_dim:\n",
    "            break\n",
    "    \n",
    "    n_users = df.user_id.unique().shape[0]\n",
    "    n_items = df.song_id.unique().shape[0]\n",
    "    sparsity = 1- (float(df.shape[0]) / float(n_users*n_items))\n",
    "    print('Number of users: {}'.format(n_users))\n",
    "    print('Number of songs: {}'.format(n_items))\n",
    "    print('Sparsity: {:.5%}'.format(sparsity))\n",
    "    return df, sparsity\n",
    "\n",
    "\n",
    "for i in range(5,50):\n",
    "    print('Min user and song count: {}'.format(i))\n",
    "    _, sparsity = activity_thresh(df, i, i)\n",
    "    if sparsity < 0.995:\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "# 25 is the optimal number\n",
    "\n",
    "\n",
    "df_red, _ = activity_thresh(df, 25, 25)\n",
    "\n",
    "\n",
    "\n",
    "# Encode relevent variables\n",
    "\n",
    "\n",
    "df_red['user_id'] = df_red['users'].astype(\"category\").cat.codes\n",
    "df_red['song_id'] = df_red['songs'].astype(\"category\").cat.codes\n",
    "\n",
    "# Create lookup for songs and artists based off eoncodes variables\n",
    "\n",
    "song_map = df_track[['songs', 'song_artist']].drop_duplicates()\n",
    "song_map['songs'] = song_map['songs'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create item/user and user/item sparse matrices\n",
    "\n",
    "sp_item_user = sp.csr_matrix((df_red['play_count'].astype(float), (df_red['user_id'], df_red['song_id'])))\n",
    "sp_user_item = sp.csr_matrix((df_red['play_count'].astype(float), (df_red['song_id'], df_red['user_id']))\n",
    "\n",
    "\n",
    "\n",
    "train_2, val_set_2, user_list_train_2 = train_test_mask(sp_item_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use alternating Least Squares to create recommendation algorithm\n",
    "\n",
    "alpha = 15\n",
    "rec = implicit.als.AlternatingLeastSquares(factors = 50, regularization = 0.1,\n",
    "                                           iterations = 20)\n",
    "rec.fit(train_2 * alpha)\n",
    "\n",
    "\n",
    "user_vec, item_vec = rec.item_factors, rec.user_factors.transpose()\n",
    "\n",
    "\n",
    "\n",
    "area_under_curve(train_2, user_lst_train_2, \n",
    "              [sp.csr_matrix(user_vec), sp.csr_matrix(item_vec)], val_set_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts -- Next Steps\n",
    "\n",
    "Our AUC score improved but still did not exceed the popularity score. To improve our model further, we will need to perform hyperparameter tuning. However, this requires more performance than a standard desktop allows for--running the AUC curve function took over a day on my machine. We will perform the rest of this project using Spark. Spark's distributed nature allows for much better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- Recommender Systems; Charu C. Aggarwal; 2016\n",
    "- Advanced Analytics with Spark; Sean Owen; 2017\n",
    "- http://yifanhu.net/PUB/cf.pdf\n",
    "- https://jessesw.com/Rec-System/\n",
    "- https://github.com/benfred/implicit\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
